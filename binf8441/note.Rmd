---
title: "binf8441"
author: yue wu
date: march 1, 2018
output:
  html_document: #ioslides_presentation
    toc: true
    toc_depth: 3
params:
  n: 100
---

```{r setup, include=FALSE}
#eval=FALSE echo=FALSE include=FALSE
knitr::opts_chunk$set(echo = TRUE)
```
## hypothersis testing


H0 and H1--> error(symmetric)--> test with small error

```{r cars, echo=FALSE}
require(knitr)
mat=matrix(c("","Type I error","Type II error",""),nrow=2,ncol=2)
colnames(mat)=c("H0","H1")
rownames(mat)=c("accepting","rejecting")
kable(mat)
```

We will control type I error

If type I error is small, typically $\leq 0.05$ we reject H0/accepting H1/ **H1 is true**

If type I error is large $\geq 0.05$ we **cannot reject H0**, However, it is type II error that are about accepting H0, if it is not known, we can not say H0 is true statistically.

can not rejecting H0 $\neq$ accepting H0

$\alpha$ H1 is true <==> accepting H1 <==> rejecting H0

**always put the expected as alternative hypothersis H1, as this way you can tell whether H1 is true(because we control type I error)**

There are three major component in a hypothesis test

1. H0 and H1. we always put the expected significant conclusion in H1


2. Ho and H1 are the hypotheses about **parameters** (**population**), not sample statistics(such as mean, already observe, and pretty sure) or specific case of inference/estimation


3. test statistics, **summarize** the data


4. Rejection region: decision rule for Rejecting H0. we calculating the threhold(critical value) $\alpha$, by controling type I error. To find type I error, we need to find the prob distr of the test statistics given theat H0 is true. This distribution is also called the null dist of the test statistics.

H_0: $\mu_1 = \mu_2$ H_1: $\mu_1 \geq \mu_2$

$\rightarrow$ H_0: $\mu1 - \mu2 = 0$ H_1: $\mu1 - \mu2 < 0$

It is reasonable to use test statistics $\overline{X_1}-\overline{X_2}$

we reject $H_0$ if $\overline{X_1}-\overline{X_2} \leq$  a Type I error $P(\text{rejecting $H_0$}|\text{$H_0$ is true})=P(\overline{X_1}-\overline{X_2} \leq \alpha|\mu_1=\mu_2)$

If we assume normality, $X_1^i \sim Normal{(\mu_1, \sigma_1)}, X_2^i \sim Normal(\mu_2, \sigma_2)$

r.v. $Y=\overline{X_1}-\overline{X_2}$

#### different cases

##### Same mean same variance

$H_0$:
\[
   X_1^i\, X_2^i \sim \text{iid} \sim Normal{(\mu,\sigma^2)}\\
  \overline{X_1}-\overline{X_2} \sim iid \sim Normal{(0,2\sigma^2/n)}
\]

we want to contorl Type I error $\leq 5\%$
\[
P(\overline{X_1}-\overline{X_2} \leq a \  | \  \mu_1=\mu_2)
\]
a is the 5% quantile of the null dist of the test statistics

##### if the sample size not equal
\[
  Var(\overline{X_1}-\overline{X_2})=Var(\overline{X_1})-Var(\overline{X_2})=\sigma^2/n+\sigma^2/m
\]
$\sigma^2$ need to be esimated, if same variance use whole population
\[
\sigma^2=\text{sample variance}=\frac{1}{n+m-1} \sum_{i=1}^{n+m}{(x_i-\overline{x})^2}
\]
if not, separately do it for each group
same \overline different variance

t statistics $\frac{\overline{X_1}-\overline{X_2}}{var(\overline{X_1}-\overline{X_2})}$ will be a $Normal{(0,1)}$ standardized

\[
  H_0 : \  \mu_1 = \mu_2 \
  H_1 : \  \mu_1 \neq \mu_2
\]

It is reasonable to use test statistics $\overline{X_1}-\overline{X_2}$

we reject $H_0$ if $\overline{X_1}-\overline{X_2} \leq a$ or $\overline{X_1}-\overline{X_2} \geq b$

Type I error
\[
= P(\text{rejecting }H_0 \  | \  H_0\text{ is true})=P(\overline{X_1}-\overline{X_2}) \leq a \   \overline{X_1}-\overline{X_2} \geq b \  | \  \mu_1=\mu_2)=P(\overline{X_1}-\overline{X_2} \leq a \  | \  \mu_1=\mu_2)+P(\overline{X_1}-\overline{X_2} \geq b \  | \  \mu_1=\mu_2)
\]

\[
  \text{add assumption: }P(\overline{X_1}-\overline{X_2} \leq a \  | \  |\mu_1=\mu_2) = P(\overline{X_1}-\overline{X_2} \geq a \  | \  \mu_1=\mu_2)
\]
a is 2.5% quantile of the null distr of $\overline{x_1}-\overline{x_2}$
b 97.5%

The hardest part: the **Null distribution** of test statistics

Rejection region: we reject $H_0$ if t > a, where a is 95% (upper tail) quantile of the null dist of t

P value: $P=P(t > t_{obs} \  | \  H_0)$ (observed test statistics): we reject $H_0$ if P-value $\leq \alpha$ such as 5% $alpha$ is prespecified   pvalue is estimation of type I error

Two side test:

$H_0 \  \mu_1 = \mu_2 \  \mu_1 \neq \mu_2$

$t = \overline{X_1} - \overline{X_2}$

rejection region we reject $H_0$ if $t<a$ or $t>b$

where a(b) is 2.5%(97.5%) quantile of the null distri of t

P-value $=2P(t<t_{obs} \  | \  H_0)$ ($=2P(t>t_{obs} \  | \  H_0)$) if $t_obs$ is negative(positive)


### Compare performance
evalution the performance of a hypothesis testing

both test need to control same type I error

Power $=P(\text{reject }H_0 \  | \  H_1)=1- \text{Type II error} = P(t>a \  | \  H_1)$
(H1 might be a interval-->power is a curve-->increase with distance)

which depends on the **alternative distribution** of t. When the alternative hypothesis is an interval, we need to calculate power for every possible value of the parameter. Thus the power is a function of the population of the parameter in H1.

MP test(most powerful test): likelihood ratio test

#### Two sample t test

$H_0 \  \mu_1 = \mu_2 \ H_1 \  \mu_1 \leq \mu_2$

$t = \frac{\overline{X_1}-\overline{X_2}}{sd(\overline{X_1} - \overline{X_2})}$

$\overline{X_1}-\overline{X_2}$ observed evidence

$sd(\overline{X_1}-\overline{X_2})$ sampling effect

significant(differences observe is not caused by sampling)

rejection region t > a, a is 95% quantile of the null distr(t distribution)

#### F test
can multiple groups all equal or not

can test variance:

$H_0: \sigma_1^2 = \sigma_2^2 \  H_1: \sigma_1^2 \neq \sigma_2^2$

can use differences or ratio using ratio There

$F=\frac{S_x^2}{S_y^2}$
$S=\frac{1}{n-1}\sum_{i-1}^{n}{(X_i-\overline{X})^2}$

rejection region F > a or F < b
where a is the 97.5% quantile of the null distr(F distribution)

$F(\alpha_1=n-1,\alpha_2=m-1)$

P-value $2P(F>F_{obs}|H_0)$ if $F_{obs}>1$ ...

#### association test
```{r smok, echo=FALSE}
require(knitr)
mat=matrix(c("6","3","2","9"),nrow=2,ncol=2)
colnames(mat)=c("smoking","nonsmoking")
rownames(mat)=c("male","female")
kable(mat)
```

$H_0$ : gender and smoking are independent <==> $P(\text{male and smoking})=P(\text{male})P(\text{smoking})$
$H_1$ : gender and smoking are associated/not independent

smoking and gender are r.v.

observed and expected value

test statistics $\chi^2=\sum{\frac{(O-E)^2}{E}}$
assum poisson mean=variance if large enough normal distr

always compare observation and expected from $H_0$
$t=\sum_{i=1}^{c}{\sum_{j=1}^{r}{\frac{(o_{ij}-e_{ij})^2}{e_{ij}}}}$ expected value are not always integers, assuming data follow poisson distr, varince==mean $e_{ij}$ is variance because $(o_{ij}-e_{ij})^2$ is squared level

comparing: t test $mean/sd$

Rejection region: we reject $H_0$ if t > a (critical value) where a is 95% quanitle of null distr()

P value: we reject $H_0$ if p-value $\leq$ 0.05 $P(t>t_{obs} | H_0)$

null distr is always needed/difficult
$df=(r-1)(c-1)$ row column

if $X$ ~ $Normal$ approximately $X^2$ ~ $\chi^2$ heavy tail more in tail, non-symmetric/ based on distance/ sample size large enough(every individual count) $<5$ for one would be small
rejection region: $\chi^2>a$ where is the 95% quantile of the null distr($\chi^2$ distri with $df ==(\text{# of column - 1})(\text{# of row - 1)}$)

P-value: $P(\chi^2>\chi^2_{obs}|H_0)$

EXP:

ATTCGATCGTACTGT assume no correlation between nearby nt

$H_0 P(A)=P(C)=P(T)=P(G)=0.25$

as long as it make sense and you can find null distr

count follow multinomial distr
cared about one it is binomial ditr n large --> poisson ditr --> squared $\chi^2$

rejection region: we reject H_0 if t>a where a is $1-\alpha$ quantile of the null distr of t

test the dimmer estimate the proportion from data use $\chi^2$

#### Fisher exact test:

assuming the marginal count are fixed(not r.v. not the case in $\chi^2$), and the r.v. is number of male who smoke(only one single r.v. in 2X2 table)

The Prob distr of the r.v. is hypergeometric distr
Pvalue=P(6)+P(7)+P(8)
$P(7)=\frac{\binom{8}{7} \cdot \binom{12}{2}}{\binom{20}{9}}$

#### likelihood ratio test (LRT)
1. two point test:
   $H_0$ $\theta=\theta_0$ vs $H_1$ $\theta=\theta_1$

   $t=\frac{l_0}{l_1} = \frac{l(\theta=\theta_0)}{l(\theta=\theta_1)}$

   assume: x1..xn ~ iid ~ $normal(\theta,\sigma^2)$

   l=f(x1..xn|$\theta$,$\sigma^2$)= $l_0|\theta=0$

   measure the extent the model fit the data

   rejection region: we reject the null hypotheses if t $\leq$ a, where a is the 5% quantile of the null distr

2. Neyman-Pearson Lemma: LRT for **two poins** hypothesis is the most **power** test

3. The hypotheses are intevals(most cases we didn't have two exact value to compare)

  $H_0$ $\theta \in \Omega_0$  $H_1$ $\theta \in \Omega_1$  no overlap

  $t=\frac{l_0}{l_1} = \frac{l(\hat{\theta_{mle}}|\Omega_0)}{l(\hat{\theta_{mle}}|\Omega_1)}$

  maximize the likelihood in the constraint region

  average and others can be used for other...

  rejection reigon: we reject the null hypotheses if t $\leq$ a, where a is the 5% quantile of the null distr

4. Nested hypothesis: the null hypothesis(reduced) is nested in the alternative hypothesis(complex/almost everthing)

  $H_0$ $M_0$ $H_1$ $M_1$

  $t=\frac{l_0}{l_1}$ => $-2\log(t)=2(\log(l_1)-\log(l_0))$ we know the distr

  where $l_0=l(\hat{\theta_{mle}}|M_0)$ and $l_1=l(\hat{\theta_{mle}}|M_1)$

  the null distr of $-2\log(t)$ is asymptotically the $\chi^2$ ditr with $df=$ # of parameter in $M_1$ - # of parameter in $M_0$

  Rejection region: we reject the null hypotheses $H_0$ if T $\geq$ a, where a is the 95% quantile of the null distr ($\chi^2$)

  $\chi^2$ depends on nested though the distribution might be abstract, while the first two cases are with abstract distr rather than $\chi^2$

1. test statistics should be the estimator for the paramter but which one? (1) compare observe with expected(null distr) (2) don't known expected(such as sequence) using likelihood ratio test
2. null distribution: bostraping

#### non parametric test
no assume distribution

Mann-Whitney test(<==> two sample t test):

$H_0$: $Pop1=Pop2$ $H_0$: not equal  whether two sample from same distri/population

t ranking all obs from min(1) to max for ties assign to each the average rank
$t=\sum_{i=1}^{n}{R(X_i)}$

Rejection region: we reject the $H_0$ if t>a or t< b where a is 97.5% and b is 2.5% quantile of null distr of t. {there is distributon in a table} or combining the two group and resampling

don't have to assume distribution rank is easier to generate/ be robust to outliers (robust statistics)

#### multiple test

Bonfferroni method: if there are n test, the overall $\alpha$ is 5% then the $\alpha$ of a single test is $\frac{5\%}{n}$ two conservative because it conrol type I error too hard

change the rule FDR(False discovery rate) not care about type I error


FDR: false discovery rate $FDR=E(\frac{\text{# of wrong rejections}}{\text{# of rejections}})$ FDR 5%. difficult to calculate but algorithm to approximate FDR

The Benjamin-Hochberg procedure can be used to control FDR

1. calculate P-value for each test and order them from the minimum to maximum $P_{(1)}...P_{(n)}$
2. for given $\alpha$ find the **largest** k such that $P_{(k)} \leq \frac{k}{n} \alpha$ {largest from the end}
3. Reject the null hypothesis for i=1,2...k

$FDR \neq \alpha$


## Monte Carlo simulation and Bootstrap(numeric approach)

{test: null distribution/estimation theory: variance of point estimator}

$\hat{\theta}$ is an estimator of parameter $\theta$ --> $Var(\hat{\theta})$

1. Monte Carlo simulation{assume population is given}, i the population/distribution is given, we can generate multiple samples from the population and use the generated samples to approxiamate $Var(\hat{\theta})$

  Example: $X_i$ ~ iid ~ $Normal(\mu=0,\sigma^2=1)$

>> $\hat{\theta}$ sample median sample size=10 (same size as original sample, because are dealing with variance)

>> we first generate samoles and each sample should have 10 observations,

>> generate 100 samples, $Var(\hat{\theta})$ ~~ the sample variance of $\hat{\theta}_i$

>> can also used for like probability $P(\hat{\theta})<3$ expectation $E(\hat{\theta})$

2. the population not give, assume distribution--> estimate parameter (parametric bootstrap), or pretend the origainal data as the population

parametric bootstrap estimate unknown parameters from data, generate samples from the population distribution with the unkown parameter being replaced by their estimates. each bootstrap have exactly the same sample size as the orgianal data does. for each bootstrap sample we caluclate the point estimate $\hat{\theta}$, we use the sample variance of $\hat{\theta}$ to estimate $Var(\hat{\theta})$ bootstrap sample(200~1000)

3. non-parametric bootstrap: don't know the distribution, regard the original data as population and generate bootstrap sample by resampling the original data with replacement.

**Examples**:

Sequence==>tree, use bootstrap to find the variation of the tree estimate $\hat{T}$

parametric bootstrapping: (mle parameter of tree(Phyml Raxml)) estimate parameters from the sequence data. the parameters include tree topology, brach length, and other parameter in the pylogenetic model --> simulate bootstrap samples(5 column)--> estimate $T_i$

non-parametric bootstrap: generate bootstrap samples by resampling the original data with replacement (iid independent the same site dependent and different site seems independent)

the model assume site are independent so have to generate this way for each site/ Can not generate each species independently

**Identify independent variable/set** considering the dependence structure

Bootstrap in hypothesis testing:

One sample test:

$H_0$ : $\mu = 0$ / $\mu_1=\mu_2$ $H_1$ : $\mu \geq 0$ / $\mu_1 \geq \mu_2$

test statistics: $t=\hat{x}$ / $X^M_1-X^M_2$  the null distribution of t?

1. 95% interval by $\hat{x}+-2sd(\hat{x})$ $\hat{x}$ can be found from sample variance
2.  assume a null distribution --> bootstraping 95% quntile of the null distribution ~ 95% qunatile of (t1...tn) (test statistics)

  // parametic bootstrapping:
  (assume normal, estimate parameter)
    X1...Xn~iid~$normal(\mu_1,\sigma^2)$
    Y1...Ym~iid~$normal(\mu_2,\sigma^2)$
  under $H_0$ X1...Xn,Y1...Ym ~iid~ $Normal(\mu,\sigma^2)$
  $\hat{\mu}=$sample average of the pooled sample
  $\hat{\sigma^2}=$sample variance of the pooled sample
  $Normal(\mu=\hat{\mu},\sigma^2=\hat{\sigma^2})$ bootstrap calculate t, do a quantile for t1..tk the null distri of t
  Rejection region: rejuect $H_0$ if t>a, a is 95% quantile of the null distri of t (t1..tk)

  //nonparamatic bootstrap: resample data from X+Y pooled data to X(N) Y(M) also called permutation test -> quantile -> rejection region

no-assumption on distr--> nonparamatic

evaluate performance: simulation(as real data not known the truth/control)+real data()


#### Linear regression model
1. Simple linear regression model(**assum** linear relationship):

    $Y_i=a+bX_i+\epsilon_i$

    Y: response/dependent variable

    X: explanatory/independent variable

    $\epsilon$: error term $\epsilon_i$ ~ iid ~ $Normal(0,\sigma^2)$

    Data: X,Y

    Parameters: a, b, $\sigma^2$

    We assume that X is fixed(not a **random variable**), but Y is a **random variable**, a and b are the two unknown parameters of the linear model(estimation) {  **independent** is from sampling, while **fixed** means that for the obtained x value and x without error}

    $Y_i$ ~ $Normal(a+bx_i,\sigma^2)$

    **ESTIMATION**: **MLE** likelihood $f(Y_1..Y_n | a,b,\sigma^2)=\prod_{i}^{n}{f(Y_i | a,b,\sigma^2)}=\prod_{i}^{n}{\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i-a-bx_i)^2}{2\sigma^2}}}$  {assum independent}

    $ll=n\log{\frac{1}{\sqrt{2\pi\sigma^2}}}-\frac{1}{2\sigma^2} \sum_{i}^{n}{(y_i-a-bx_i)^2}=f(\sigma^2)-g(\sigma^2)h(a,b)$  turn out max with a and b AND $\sigma^2$ can be separated, for each $\sigma^2$ the always

    for any fixing value of $\sigma^2$ Maximizing logL with a and b is equivalent to minizing (sum of squared errors) $SSE=\sum{(y_i-a-bx_i)^2}$ with a and b
    $(Obs-Exp)^2$ ~ $error^2$  Hence, the ll can be optimized with a and b first and produce f($\sigma^2$) (2steps)

    Before ML, least squared estimation has already been used by minizing the sum of squared errors. (MLE == LSE  under the linear model assumption)

    $\widehat{b}=\frac{\sum{(X_i-\bar{X})(Y_i-\bar{Y})}}{\sum{(X_i-\bar{X})^2}}$    r.v.    $E( \widehat{b} )=b$   $E(y_i)=a+bx_i$      $E(\bar{y})=a+b\bar{x}$ related constant **unbiased**

    $\widehat{a}=\bar{Y}-b\bar{X}$    r.v.    **unbiased**

    a and b normal distribution

    $\widehat{\sigma^2}=\frac{\sum{(y_i-\widehat{y_i})^2}}{n-2}$ depends on degree of freedom(variance average is given) unbiased r.v.

    $\widehat{y}=\widehat{a}+\widehat{b}X_i$ expected $y_i$

    RES: $Y=\widehat{a}+\widehat{b}X$ if b>0 positive b< 0 negative b=0 no **linear** relationship(might quadric relationship)

    **hypotheses test**: $H_0$: $b=0$ $H_1$: $b \neq 0$ $H_0$ nested in $H_1$

    LRT=$\frac{L_0}{L_1}$

    $L_0=f(Y_i | H_0(b=0,a_{mle},\sigma^2_{mle}))$

    $L_1=f(Y_i | a_{mle},b_{mle}, \sigma^2_{mle})$

    -2log(LRT) $\chi^2$

    test2: $t=\frac{\widehat{b}-b}{sd(\widehat{b})}=\frac{\widehat{b}}{sd(\widehat{b})}$ t distribution

    lm function in R

    $\epsilon=y-a-bx$
    residule=$\widehat{y_i}-a-b\widehat{x_i}$

    goodness of fit checking model assumption(normal?)

  >> the **error** term are accumulated/average effect of all(**many**) the other factors and they are all with **similar** effect (CLE-->**normal** distribution) check hypotheses test (goodness of fit)

  >> use residue($y_1-\widehat{y_1}$) to formulate the $\epsilon$
  >> residue plot (residue~x) mean=0 independent of each others(**no pattern**) --> model choice? (can fit y to any funcion of x as long as that form is **given**) the distribution test(normality test) depends on large sample size

##### multiple linear regression:

  $y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ $\epsilon$ ~ iid ~ $Normal(0,\sigma^2)$

  matrix denotion

  $Y=(y_i)^T$

  $X=(x_{ij})$ first column all 1 for $\beta_0$

  $\beta=(\beta_j)$

  $\epsilon$ vector

  Y=X$\beta$+$\epsilon$

  SSE=$(Y-X\beta)'(Y-X\beta)$  we estimate $\beta$ by minizing SSE (LSE)

  $\frac{dSSE}{d\beta}= -2Y'X+2X'X\beta=0$

  $X'X$ symmetric

  $\widehat{beta}_{LSE}=(X'X)^{-1}X'Y$ estimate all $\beta$ simulatiously/ same as MLE estimator

  $scalar'=scalar$

  $Y_i$ ~ $normal(\beta_0+beta_1 X_1+beta_2 X_2..., \sigma^2)$

  E($\widehat{beta}_{LSE}$)=$\beta$ **unbiased** estaimator

  $H_0$: $\beta_i=0$ $H_1$: $\beta_i \neq 0$

  remove those equal 0, no prediction power

##### transformation
  1. do the residule plot and make decision based on experiences
  2. log transformation(works for most cases) $\log(y)=a+b \log(x)$ OR $y=a+b \log(x)$
  3. box-cod transformation Y={$\frac{Y^{\lambda}-1}{\lambda}$ if $\lambda \neq 0$ $log(Y)$ if $\lambda=0$} and find optimal $lambda$ by R(SSE) (boxcox) {provide some general method}

##### variable selection
  a subset of ($X_i$) which have linear relaion with Y

  1. Forward selection: start from the simplest model with no corvariate x, and add by hypotheses test (between simpler and more complex model) unitil we cannot reject $H_0$
  2. Backward selection: we start with the full model
  3. Mixed selection:
